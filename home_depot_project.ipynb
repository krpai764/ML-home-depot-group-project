{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dlibeX_ivhUH"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from ydata_profiling import ProfileReport\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "import lightgbm as lgb\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "# Make sure you have mounted your Google Drive before running this cell.\n",
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')\n",
        "\n",
        "# Replace the file paths with the actual paths to your files in Google Drive\n",
        "try:\n",
        "    df1= pd.read_csv('/content/drive/My Drive/project/POS_CASH_balance.csv')\n",
        "    df2= pd.read_csv('/content/drive/My Drive/project/application_test.csv')\n",
        "    df3= pd.read_csv('/content/drive/My Drive/project/application_train.csv')\n",
        "    df4= pd.read_csv('/content/drive/My Drive/project/previous_application.csv')\n",
        "    df5= pd.read_csv('/content/drive/My Drive/project/bureau.csv')\n",
        "    df6= pd.read_csv('/content/drive/My Drive/project/bureau_balance.csv')\n",
        "    df7= pd.read_csv('/content/drive/My Drive/project/credit_card_balance.csv')\n",
        "    df8= pd.read_csv('/content/drive/My Drive/project/installments_payments.csv')\n",
        "    print(\"All files read successfully!\")\n",
        "except FileNotFoundError as e:\n",
        "    print(f\"Error: {e}. Please check the file paths and ensure the files exist in your Google Drive.\")\n",
        "except Exception as e:\n",
        "    print(f\"An unexpected error occurred: {e}\")\n",
        "\n",
        "\n",
        "#JUST CHECKING THE REPORT FOR THE MAIN COLUMN WITH A LOT OF SETS\n",
        "profile = ProfileReport(df1, title=\"Pandas Profiling Report\")\n",
        "profile.to_file(output_file = 'output1.html')\n",
        "\n",
        "\n",
        "sample = [df1, df2, df3, df4, df5, df6, df7, df8]\n",
        "names =['df1', 'df2', 'df3', 'df4', 'df5', 'df6', 'df7', 'df8']\n",
        "\n",
        "for name,df in zip(names,sample):\n",
        "   corr = df.corrwith(df3[\"TARGET\"], numeric_only=True)\n",
        "   print(f\"Correlation matrix for {name}:\")\n",
        "   print(corr)\n",
        "\n",
        "\n",
        "sample = [df1, df2, df3, df4, df5, df6, df7, df8]\n",
        "names = ['df1', 'df2', 'df3', 'df4', 'df5', 'df6', 'df7', 'df8']\n",
        "\n",
        "for name, df in zip(names, sample):\n",
        "    for column in df.columns:\n",
        "        null_count = df[column].isnull().sum()\n",
        "        percentage = (null_count / df.shape[0]) * 100  # Percentage of nulls in this column\n",
        "        print(f\"Percentage of null values in {name}, column '{column}': {percentage:.2f}%\")\n",
        "\n",
        "\n",
        "sample = [df1, df2, df3, df4, df5, df6, df7, df8]\n",
        "names =['df1', 'df2', 'df3', 'df4', 'df5', 'df6', 'df7', 'df8']\n",
        "for name,df in zip(names,sample):\n",
        " print(f\"\\nCount of each data type in {name} f1:\")\n",
        " display(df.dtypes.value_counts())\n",
        "\n",
        "\n",
        "# ALL work first on bureau_application\n",
        "bureau_balance_aggregate = df6.groupby(\"SK_ID_BUREAU\").agg({ \"MONTHS_BALANCE\": [\"min\", \"max\", \"mean\"] })\n",
        "bureau_balance_aggregate.columns = [\"BB_\" + \"_\".join(col).upper() for col in bureau_balance_aggregate.columns]\n",
        "bureau_balance_aggregate.reset_index(inplace=True)\n",
        "df5 = df5.merge(bureau_balance_aggregate, on=\"SK_ID_BUREAU\", how=\"left\")\n",
        "\n",
        "#ALL CHANGES NOW TO BE DONE IN BUREAU CSV\n",
        "df5_agg = df5.drop(columns=['SK_ID_CURR', 'SK_ID_BUREAU'])\n",
        "\n",
        "# Select only numeric columns\n",
        "numeric_cols_df5 = df5_agg.select_dtypes(include='number')\n",
        "\n",
        "\n",
        "agg_dict_df5 = {}\n",
        "for col in numeric_cols_df5.columns:\n",
        "    agg_dict_df5[col] = ['mean', 'max', 'min']\n",
        "\n",
        "# Aggregate df5\n",
        "bureau_aggregate_numeric = df5.groupby(\"SK_ID_CURR\").agg(agg_dict_df5)\n",
        "\n",
        "# Flatten the multi-level column names for numeric aggregation\n",
        "bureau_aggregate_numeric.columns = [\"_\".join(col).strip() for col in bureau_aggregate_numeric.columns.values]\n",
        "\n",
        "# Reset index\n",
        "bureau_aggregate_numeric.reset_index(inplace=True)\n",
        "\n",
        "\n",
        "non_numeric_cols_df5 = df5_agg.select_dtypes(exclude='number')\n",
        "\n",
        "# Define the aggregation dictionary for non-numeric columns using mode\n",
        "agg_dict_non_numeric_df5 = {}\n",
        "for col in non_numeric_cols_df5.columns:\n",
        "    agg_dict_non_numeric_df5[col] = lambda x: x.mode()[0] if not x.mode().empty else None\n",
        "\n",
        "# Aggregate non-numeric columns by SK_ID_CURR\n",
        "bureau_aggregate_non_numeric = df5.groupby(\"SK_ID_CURR\").agg(agg_dict_non_numeric_df5)\n",
        "\n",
        "# Reset index to make SK_ID_CURR a column\n",
        "bureau_aggregate_non_numeric.reset_index(inplace=True)\n",
        "\n",
        "# Merge\n",
        "bureau_aggregate_all = pd.merge(bureau_aggregate_numeric, bureau_aggregate_non_numeric, on='SK_ID_CURR', how='left')\n",
        "\n",
        "\n",
        "#ALL ON CREDIT CARD BALANCE NOW\n",
        "for column in df7.columns:\n",
        "  null_count = df7[column].isnull().sum()\n",
        "  percentage = (null_count/df7.shape[0])*100\n",
        "  print(f\"null percentage of {column} is equal to {percentage:.2f}%\")\n",
        "\n",
        "\n",
        "credit_install_agg = df7.groupby(\"SK_ID_CURR\").agg({\n",
        "    \"MONTHS_BALANCE\": [\"min\", \"max\", \"mean\"],\n",
        "    \"AMT_BALANCE\": [\"sum\", \"mean\", \"max\"],\n",
        "    \"AMT_CREDIT_LIMIT_ACTUAL\": [\"mean\", \"max\"],\n",
        "    \"SK_DPD\": [\"max\", \"mean\"],\n",
        "    \"CNT_DRAWINGS_ATM_CURRENT\": \"sum\",\n",
        "    \"CNT_DRAWINGS_CURRENT\": \"sum\",\n",
        "    \"CNT_DRAWINGS_OTHER_CURRENT\": \"sum\",\n",
        "    \"CNT_DRAWINGS_POS_CURRENT\": \"sum\",\n",
        "    \"AMT_DRAWINGS_ATM_CURRENT\": \"sum\",\n",
        "    \"AMT_DRAWINGS_CURRENT\": \"sum\",\n",
        "    \"AMT_DRAWINGS_OTHER_CURRENT\": \"sum\",\n",
        "    \"AMT_DRAWINGS_POS_CURRENT\": \"sum\"\n",
        "})\n",
        "\n",
        "credit_install_agg['SUM_ALL_CNT_DRAWINGS'] = credit_install_agg[[('CNT_DRAWINGS_ATM_CURRENT', 'sum'),\n",
        "                                                   ('CNT_DRAWINGS_CURRENT', 'sum'),\n",
        "                                                   ('CNT_DRAWINGS_OTHER_CURRENT', 'sum'),\n",
        "                                                   ('CNT_DRAWINGS_POS_CURRENT', 'sum')]].sum(axis=1)\n",
        "\n",
        "credit_install_agg['SUM_ALL_AMT_DRAWINGS'] = credit_install_agg[[('AMT_DRAWINGS_ATM_CURRENT', 'sum'),\n",
        "                                                   ('AMT_DRAWINGS_CURRENT', 'sum'),\n",
        "                                                   ('AMT_DRAWINGS_OTHER_CURRENT', 'sum'),\n",
        "                                                   ('AMT_DRAWINGS_POS_CURRENT', 'sum')]].sum(axis=1)\n",
        "\n",
        "install_agg = df8.groupby(\"SK_ID_CURR\").agg({\n",
        "    \"NUM_INSTALMENT_VERSION\": [\"nunique\"],\n",
        "    \"AMT_INSTALMENT\": [\"sum\", \"mean\", \"max\"],\n",
        "    \"AMT_PAYMENT\": [\"sum\", \"mean\", \"min\"],\n",
        "    \"DAYS_ENTRY_PAYMENT\": [\"min\", \"max\", \"mean\"]})\n",
        "\n",
        "POS_agg = df1.groupby(\"SK_ID_CURR\").agg({'MONTHS_BALANCE': ['max', 'mean'],\n",
        "        'SK_DPD': ['max', 'mean', 'sum'],\n",
        "        'SK_DPD_DEF': ['max', 'mean', 'sum'],\n",
        "        'CNT_INSTALMENT_FUTURE': ['mean', 'sum'],\n",
        "        'CNT_INSTALMENT': ['max'],\n",
        "        'NAME_CONTRACT_STATUS': lambda x: x.mode()[0] if not x.mode().empty else None})\n",
        "\n",
        "prev_application_agg = df4_copy.groupby(\"SK_ID_CURR\").agg({\n",
        "\n",
        "\n",
        "        'AMT_ANNUITY': ['min', 'max', 'mean'],\n",
        "        'AMT_APPLICATION': ['min', 'max', 'mean'],\n",
        "        'AMT_CREDIT': ['min', 'max', 'mean'],\n",
        "        'AMT_GOODS_PRICE': ['min', 'max', 'mean'],\n",
        "        'HOUR_APPR_PROCESS_START': ['min', 'max', 'mean'],\n",
        "        'CNT_PAYMENT': ['mean', 'sum'],\n",
        "        'DAYS_DECISION': ['min', 'max', 'mean'],   })\n",
        "\n",
        "df_train = df3.copy()\n",
        "df_test = df2.copy()\n",
        "df_train = df_train.merge(bureau_aggregate_all, on=\"SK_ID_CURR\", how=\"left\")\n",
        "df_test  = df_test.merge(bureau_aggregate_all, on=\"SK_ID_CURR\", how=\"left\")\n",
        "\n",
        "credit_install_agg.columns = [\"_\".join(col).strip() for col in credit_install_agg.columns.values]\n",
        "\n",
        "df_train = df_train.merge(credit_install_agg, on=\"SK_ID_CURR\", how=\"left\")\n",
        "df_test  = df_test.merge(credit_install_agg, on=\"SK_ID_CURR\", how=\"left\")\n",
        "\n",
        "\n",
        "install_agg.columns = [\"_\".join(col).strip() for col in install_agg.columns.values]\n",
        "\n",
        "df_train = df_train.merge(install_agg, on=\"SK_ID_CURR\", how=\"left\")\n",
        "df_test  = df_test.merge(install_agg, on=\"SK_ID_CURR\", how=\"left\")\n",
        "\n",
        "\n",
        "POS_agg.columns = [\"_\".join(col).strip() for col in POS_agg.columns.values]\n",
        "\n",
        "df_train = df_train.merge(POS_agg, on=\"SK_ID_CURR\", how=\"left\")\n",
        "df_test  = df_test.merge(POS_agg, on=\"SK_ID_CURR\", how=\"left\")\n",
        "\n",
        "\n",
        "prev_application_agg.columns = [\"_\".join(col).strip() for col in prev_application_agg.columns.values]\n",
        "\n",
        "df_train = df_train.merge(prev_application_agg, on=\"SK_ID_CURR\", how=\"left\")\n",
        "df_test  = df_test.merge(prev_application_agg, on=\"SK_ID_CURR\", how=\"left\")\n",
        "\n",
        "# FROM HERE ON FORGET ABT THE TEST THING FOR A WHILE I AM JUST TESTING THE DATA HERE, SO DO NOT FORGET\n",
        " #  TO MAKE THE SAME CHANGES IN TRAIN LATER FROM THE CODE USED HERE ON NOW\n",
        " # DONT FORGET\n",
        " # DONT FORGET\n",
        " # DONT FORGET THIS IS THE LINE\n",
        " # CHECK IF YOU HAVE ENCODED THE ORIGINAL TEST AND TRAIN\n",
        "\n",
        "\n",
        "X = df_train.drop(columns=[\"TARGET\"])\n",
        "y = df_train[\"TARGET\"]\n",
        "\n",
        "X_train, X_valid, y_train, y_valid = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42, stratify=y )\n",
        "\n",
        "\n",
        "label_encoders = {}\n",
        "for col in X_train.select_dtypes(include=['object']).columns:\n",
        "    le = LabelEncoder()\n",
        "    X_train[col] = le.fit_transform(X_train[col].astype(str))\n",
        "    # Apply the same encoder\n",
        "    X_valid[col] = le.transform(X_valid[col].astype(str))\n",
        "    label_encoders[col] = le\n",
        "\n",
        "\n",
        "#GRID SEARCH RIGHT NOW\n",
        "param_grid = {\n",
        "    \"learning_rate\": [ 0.03, 0.05],\n",
        "    \"num_leaves\": [32, 64],\n",
        "    \"feature_fraction\": [0.8],\n",
        "    \"subsample\": [0.8],\n",
        "    \"random_state\": [42,62],\n",
        "    \"reg_alpha\": [1.0, 0.4],  # L1 regularization\n",
        "    \"reg_lambda\": [1.0, 0.4], # L2 regularization\n",
        "}\n",
        "\n",
        "\n",
        "lgbm = lgb.LGBMClassifier(objective='binary', metric='auc')\n",
        "\n",
        "\n",
        "grid = GridSearchCV(\n",
        "    estimator=lgbm,\n",
        "    param_grid=param_grid,\n",
        "    scoring='roc_auc',\n",
        "    cv=3,\n",
        "    verbose=1,\n",
        "    n_jobs=-1  )\n",
        "\n",
        "\n",
        "grid.fit(X_train, y_train)\n",
        "\n",
        "# Best parameters and best score\n",
        "print(\"Best parameters:\", grid.best_params_)\n",
        "print(\"Best ROC AUC score (CV):\", grid.best_score_)\n",
        "\n",
        "# Evaluate on validation set (optional)\n",
        "y_pred = grid.best_estimator_.predict_proba(X_valid)[:, 1]\n",
        "roc_auc = roc_auc_score(y_valid, y_pred)\n",
        "print(\"Validation ROC AUC:\", roc_auc)\n",
        "\n",
        "\n",
        "# INITIAL MODEL THIS IS\n",
        "params = {\n",
        "    \"objective\": \"binary\",\n",
        "    \"metric\": \"auc\",\n",
        "    \"learning_rate\": 0.05,\n",
        "    \"num_leaves\": 32,\n",
        "    \"feature_fraction\": 0.8,\n",
        "    \"subsample\": 0.8,\n",
        "    \"random_state\": 42,\n",
        "    \"lambda_l1\": 1.0,  # L1 regularization\n",
        "    \"lambda_l2\": 1.0   # L2 regularization\n",
        "}\n",
        "\n",
        "# Prepare LightGBM datasets\n",
        "train_data = lgb.Dataset(X_train, label=y_train)\n",
        "valid_data = lgb.Dataset(X_valid, label=y_valid, reference=train_data)\n",
        "\n",
        "# Train the model with early stopping THIS REMEMBER WILL BE HELPFUL TOOK TIME\n",
        "model = lgb.train(\n",
        "    params,\n",
        "    train_data,\n",
        "    valid_sets=[valid_data],\n",
        "    callbacks=[lgb.early_stopping(stopping_rounds=50), lgb.log_evaluation(0)]\n",
        ")\n",
        "\n",
        "# Predict probabilities for the validation set\n",
        "y_pred = model.predict(X_valid)\n",
        "\n",
        "# Calculate ROC AUC\n",
        "roc_auc = roc_auc_score(y_valid, y_pred)\n",
        "print(\"Validation ROC AUC:\", roc_auc)\n",
        "\n",
        "\n",
        "#CHECKING THE IMPORTANCE OF EACH\n",
        "\n",
        "importances = model.feature_importance()\n",
        "feature_names = X_train.columns\n",
        "\n",
        "\n",
        "importance_df = pd.DataFrame({\n",
        "    'feature': feature_names,\n",
        "    'importance': importances\n",
        "})\n",
        "\n",
        "\n",
        "importance_df = importance_df.sort_values(by='importance', ascending=False)\n",
        "\n",
        "print(importance_df)\n",
        "\n",
        "\n",
        "importance_df['percentage'] = importance_df['importance'] / importance_df['importance'].sum() * 100\n",
        "importance_df.head()\n",
        "\n",
        "\n",
        "from os import PRIO_PGRP\n",
        "print(importance_df['importance'].sum())\n",
        "filt = importance_df['percentage']<2\n",
        "print(importance_df.loc[filt].count())\n",
        "\n",
        "\n",
        "#THIS MEANS ALL COLUMNS ARE MORE OR LESS IMPORTANT AS ALL COLUMNS HAVE IMPORTANCE LESS THAN 6 PERCENT\n",
        "#HERE ON DOING IT FOR THE TEST COLUMN\n",
        "label_encoders = {}\n",
        "for col in df_test.select_dtypes(include=['object']).columns:\n",
        "    le = LabelEncoder()\n",
        "    df_test[col] = le.fit_transform(df_test[col].astype(str))\n",
        "    label_encoders[col] = le  # Optionally save the encoder for inverse_transform later\n",
        "\n",
        "\n",
        "label_encoders = {}\n",
        "for col in df_train.select_dtypes(include=['object']).columns:\n",
        "    le = LabelEncoder()\n",
        "    df_train[col] = le.fit_transform(df_train[col].astype(str))\n",
        "    label_encoders[col] = le\n",
        "\n",
        "\n",
        "\n",
        "Xtrainfin = df_train.drop(columns=['TARGET'])\n",
        "Ytrainfin = df_train['TARGET']\n",
        "\n",
        "import lightgbm as lgb\n",
        "\n",
        "\n",
        "model2 = lgb.LGBMClassifier(\n",
        "    objective=\"binary\",\n",
        "    metric=\"auc\",\n",
        "    learning_rate=0.05,\n",
        "    num_leaves=32,\n",
        "    feature_fraction=0.8,\n",
        "    subsample=0.8,\n",
        "    random_state=42,\n",
        "    reg_alpha=1.0,   # L1 regularization\n",
        "    reg_lambda=1.0   # L2 regularization\n",
        ")\n",
        "\n",
        "# Fit the model to your training data\n",
        "model2.fit(Xtrainfin, Ytrainfin)\n",
        "\n",
        "\n",
        "y_test_pred = model2.predict_proba(df_test)[:, 1]\n",
        "output = pd.DataFrame({'SK_ID_CURR': df_test['SK_ID_CURR'], 'TARGET': y_test_pred})\n",
        "output.to_csv('submission.csv', index=False)"
      ]
    }
  ]
}